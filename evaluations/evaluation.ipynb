{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff8fcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from contextlib import AbstractContextManager\n",
    "from itertools import groupby\n",
    "from typing import Dict, List\n",
    "\n",
    "import weaviate\n",
    "from openai import OpenAI\n",
    "from weaviate.classes.query import Filter, HybridFusion, MetadataQuery\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModel.from_pretrained(model_path, add_pooling_layer=True)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs)[0][:, 0]\n",
    "        return torch.nn.functional.normalize(embeddings, p=2, dim=1).squeeze(0).tolist()\n",
    "\n",
    "# Initialize models \n",
    "base_dir = os.getcwd()\n",
    "embedding_path = os.path.join(base_dir, \"embeddings\")\n",
    "# For embedding model\n",
    "model_manager = ModelManager(embedding_path)  \n",
    "# For classification, query transformation, generation, and validation\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "class RetrievalDecisionModule:\n",
    "    def __init__(self, client=client, model=\"gpt-4o-mini\", temperature=0.0, max_tokens=5):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def classify_if_retrieval_needed(self, user_input, context_str):\n",
    "        prompt = f\"\"\"\n",
    "        You are an assistant helping decide whether a user message needs document retrieval or not.\n",
    "\n",
    "        Instructions:\n",
    "        - If the user's message is purely conversational (e.g. \"hi\", \"thanks\", \"that's helpful\") or can be answered from previous chat messages or common knowledge (e.g. general facts), respond: **\"no\"**.\n",
    "        - If the message requires external knowledge, document retrieval, or detailed information not provided in the chat history, respond: **\"yes\"**.\n",
    "        - If the user's message is a simple factual question (e.g., \"What is the capital of France?\" or \"How many days are in a week?\"), respond: **\"no\"**.\n",
    "        \n",
    "        Chat History:\n",
    "        {context_str}\n",
    "\n",
    "        User Input:\n",
    "        {user_input}\n",
    "\n",
    "        Does this query need document retrieval? (Answer only \"yes\" or \"no\")\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.strip()}],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content.strip().lower() == \"yes\"\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "\n",
    "class QueryTransformationModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client=client,\n",
    "        refine_model=\"gpt-4o-mini\",\n",
    "        hyde_model=\"gpt-4o-mini-search-preview\",\n",
    "        refine_temperature=0.3,\n",
    "        hyde_temperature=0.7,\n",
    "        refine_max_tokens=300,\n",
    "        hyde_max_tokens=512\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.refine_model = refine_model\n",
    "        self.hyde_model = hyde_model\n",
    "        self.refine_temperature = refine_temperature\n",
    "        self.hyde_temperature = hyde_temperature\n",
    "        self.refine_max_tokens = refine_max_tokens\n",
    "        self.hyde_max_tokens = hyde_max_tokens\n",
    "\n",
    "    def refine_query_with_history(self, new_query, context_str):\n",
    "        prompt = f\"\"\"\n",
    "        You're legal query refiner helping create effective search queries for Quezon City documents. Consider both:\n",
    "        1. The new user query\n",
    "        2. Relevant context from chat history (if applicable)\n",
    "\n",
    "        Your task:\n",
    "        - Refine the query into a standalone, clear, **affirmative sentence** (not a question), in English, suitable for document search.\n",
    "\n",
    "        Chat History (most recent first): {context_str}\n",
    "\n",
    "        New Query: {new_query}\n",
    "\n",
    "        Refined Search Query (respond ONLY with the refined query in ENGLISH):\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.refine_model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.strip()}],\n",
    "                temperature=self.refine_temperature,\n",
    "                max_tokens=self.refine_max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception:\n",
    "            return new_query\n",
    "\n",
    "    def generate_hypothetical_document(self, query: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        You are a legal research assistant for Quezon City. Based on the query below, generate an answer that could plausibly address it.\n",
    "\n",
    "        - The content should be realistic and relevant.\n",
    "        - Translate to English if needed.\n",
    "\n",
    "        Query: \"{query}\"\n",
    "\n",
    "        Hypothetical legal document (1 paragraph):\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.hyde_model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt.strip()}\n",
    "                ],\n",
    "                max_tokens=self.hyde_max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception:\n",
    "            return \"Error generating hypothetical document\"\n",
    "\n",
    "\n",
    "class DocumentRetrievalModule(AbstractContextManager):\n",
    "    def __init__(self, host=\"localhost\", collection_name=\"BAAI\", alpha=0.5, context_window=1):\n",
    "        self.host = host\n",
    "        self.collection_name = collection_name\n",
    "        self.alpha = alpha\n",
    "        self.context_window = context_window\n",
    "        self.client = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.client = weaviate.connect_to_local(host=self.host)\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        if self.client:\n",
    "            self.client.close()\n",
    "\n",
    "    def search_documents(self, query_text: str, max_results: int) -> List[Dict]:\n",
    "        try:\n",
    "            self.collection = self.client.collections.get(self.collection_name)\n",
    "            response = self.collection.query.hybrid(\n",
    "                query=query_text,\n",
    "                vector=model_manager.get_embedding(query_text),\n",
    "                alpha=self.alpha,\n",
    "                fusion_type=HybridFusion.RELATIVE_SCORE,\n",
    "                limit=max_results,\n",
    "                return_properties=[\"text\", \"source\", \"category\", \"chunk_index\"],\n",
    "                return_metadata=MetadataQuery(score=True)\n",
    "            )\n",
    "\n",
    "            results = []\n",
    "            seen = set()\n",
    "\n",
    "            for obj in response.objects:\n",
    "                key = (obj.properties[\"source\"], obj.properties[\"category\"], obj.properties[\"chunk_index\"])\n",
    "                if key not in seen:\n",
    "                    seen.add(key)\n",
    "                    results.append({\n",
    "                        \"text\": obj.properties[\"text\"],\n",
    "                        \"source\": obj.properties[\"source\"],\n",
    "                        \"category\": obj.properties[\"category\"],\n",
    "                        \"chunk_index\": obj.properties[\"chunk_index\"],\n",
    "                        \"score\": obj.metadata.score\n",
    "                    })\n",
    "\n",
    "            merged_results = self.expand_documents(results)\n",
    "            return merged_results\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": \"Error searching for documents\", \"details\": str(e)}\n",
    "\n",
    "    def expand_documents(self, results: List[Dict]) -> List[Dict]:\n",
    "        merged_results = []\n",
    "        if results:\n",
    "            expanded_chunks = self.expand_document_search(results)\n",
    "            seen = {(chunk['source'], chunk['category'], chunk['chunk_index']) for chunk in results}\n",
    "\n",
    "            for chunk in expanded_chunks:\n",
    "                if (chunk['source'], chunk['category'], chunk['chunk_index']) not in seen:\n",
    "                    results.append({\n",
    "                        \"text\": chunk[\"text\"],\n",
    "                        \"source\": chunk[\"source\"],\n",
    "                        \"category\": chunk[\"category\"],\n",
    "                        \"chunk_index\": chunk[\"chunk_index\"],\n",
    "                        \"score\": 0})\n",
    "\n",
    "            category_order = {\n",
    "                'Introduction': 0,\n",
    "                'Preamble': 1,\n",
    "                'Operative': 2,\n",
    "                'Signature': 3,\n",
    "                'Uncategorized': 4\n",
    "            }\n",
    "\n",
    "            sorted_items = sorted(\n",
    "                results,\n",
    "                key=lambda x: (\n",
    "                    x['source'].lower(),\n",
    "                    int(category_order.get(x['category'], 4)),\n",
    "                    int(x['chunk_index'])\n",
    "                )\n",
    "            )\n",
    "\n",
    "            for (source, category), group in groupby(sorted_items, key=lambda x: (x['source'], x['category'])):\n",
    "                group = list(group)\n",
    "                group.sort(key=lambda x: x['chunk_index'])\n",
    "\n",
    "                merged = [group[0]]\n",
    "                for item in group[1:]:\n",
    "                    last = merged[-1]\n",
    "\n",
    "                    if item['chunk_index'] == last['chunk_index'] + 1:\n",
    "                        last['text'] += item['text']\n",
    "                        last['score'] = max(last['score'], item['score'])\n",
    "                        last['chunk_index'] = item['chunk_index']\n",
    "                    else:\n",
    "                        merged.append(item)\n",
    "                merged_results.extend(merged)\n",
    "        return sorted(merged_results, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    def expand_document_search(self, initial_results: List[Dict]) -> List[Dict]:\n",
    "        expanded_chunks = []\n",
    "        doc_sources = set()\n",
    "\n",
    "        for chunk in reversed(initial_results):\n",
    "            doc_sources.add((chunk['source'], chunk['category'], chunk['chunk_index']))\n",
    "\n",
    "        try:\n",
    "            for source, category, index in doc_sources:\n",
    "                filters = (\n",
    "                    Filter.by_property(\"source\").equal(source)\n",
    "                    & Filter.by_property(\"category\").equal(category)\n",
    "                    & Filter.by_property(\"chunk_index\").greater_than(index - self.context_window - 1)\n",
    "                    & Filter.by_property(\"chunk_index\").less_than(index + self.context_window + 1)\n",
    "                )\n",
    "\n",
    "                response = self.collection.query.fetch_objects(\n",
    "                    filters=filters,\n",
    "                    return_properties=[\"text\", \"source\", \"category\", \"chunk_index\"]\n",
    "                )\n",
    "\n",
    "                for obj in response.objects:\n",
    "                    expanded_chunks.append({\n",
    "                        \"text\": obj.properties[\"text\"],\n",
    "                        \"source\": obj.properties[\"source\"],\n",
    "                        \"category\": obj.properties[\"category\"],\n",
    "                        \"chunk_index\": obj.properties[\"chunk_index\"],\n",
    "                        \"score\": 0\n",
    "                    })\n",
    "            return expanded_chunks\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": \"Error expanding document search\", \"details\": str(e)}\n",
    "\n",
    "\n",
    "class ResponseGeneratorModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client=client,\n",
    "        generation_model_with_retrieval=\"gpt-4o\",\n",
    "        generation_model_without_retrieval=\"gpt-4o-mini-search-preview\",\n",
    "        generation_temperature=0.1,\n",
    "        max_tokens=512\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.generation_model_with_retrieval = generation_model_with_retrieval\n",
    "        self.generation_model_without_retrieval = generation_model_without_retrieval\n",
    "        self.generation_temperature = generation_temperature\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def conversation_without_retrieval(self, user_input, context_str=None):\n",
    "        prompt = f\"\"\"\n",
    "        You are a Quezon City Legal Provider. Answer the query using your internal knowledge or the provided conversation history if applicable.\n",
    "\n",
    "        Conversation history:\n",
    "        {context_str if context_str else 'No previous conversation history.'}\n",
    "\n",
    "        User query:\n",
    "        {user_input}\n",
    "\n",
    "        Please answer clearly and accurately.  \n",
    "        Note: If the user query is in English, provide the answer in English. If the query is in Filipino, provide the answer in Filipino.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.generation_model_without_retrieval,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.strip()}],\n",
    "                temperature=self.generation_temperature,\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception:\n",
    "            return \"Error generating response\"\n",
    "\n",
    "\n",
    "    def generate_response(self, query, context_docs, context_sources):\n",
    "        context_str = \"\\n\\n\".join(\n",
    "            [f\"Document {index + 1}:\\n{doc['text']}\" for index, doc in enumerate(context_docs)]\n",
    "        )\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        You are a legal AI assistant helping users find information from ordinances and resolutions. \n",
    "        Answer the query **strictly using the provided context below**. \n",
    "\n",
    "        If you use any context in your answer, you must clearly indicate which document(s) you used **using the format: \"Document X\"** (e.g. Document 1, Document 2).\n",
    "\n",
    "        Query: {query}\n",
    "\n",
    "        Context: {context_str if context_docs else 'No relevant documents found.'}\n",
    "\n",
    "        Note: If the user query is in English, provide the answer in English. If the query is in Filipino, provide the answer in Filipino.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.generation_model_with_retrieval,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.strip()}],\n",
    "                temperature=self.generation_temperature,\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "\n",
    "            generated_answer = response.choices[0].message.content.strip()\n",
    "\n",
    "            doc_numbers = list(set(re.findall(r\"document\\s+(\\d+)\", generated_answer, re.IGNORECASE)))\n",
    "            relevant_sources = [context_sources[int(num) - 1] for num in doc_numbers if num.isdigit()]\n",
    "            relevant_contexts = [context_docs[int(num) - 1] for num in doc_numbers if num.isdigit()]\n",
    "            generated_answer = re.sub(\n",
    "                r\"(\\(?\\s*(See\\s+)?(Sources?:\\s*)?(Document\\s+\\d+[,\\s]*)+(and\\s+)?(Document\\s+\\d+)?\\s*\\)?)\", \n",
    "                \"\", \n",
    "                generated_answer, \n",
    "                flags=re.IGNORECASE\n",
    "            ).strip()\n",
    "\n",
    "            return generated_answer, relevant_sources, relevant_contexts\n",
    "        except Exception:\n",
    "            return \"Error generating response\", [], []\n",
    "\n",
    "\n",
    "class AnswerValidationAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client=client,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0.0,\n",
    "        max_tokens=512,\n",
    "        max_attempts=3\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.max_attempts = max_attempts\n",
    "        self.current_attempt = 0\n",
    "        self.search_params = {'max_results': 7, 'alpha': 0.5}\n",
    "\n",
    "    def validate_answer(self, answer, context_docs, query):\n",
    "        if not context_docs:\n",
    "            return False\n",
    "\n",
    "        context_str = \"\\n\\n\".join(doc['text'] for doc in context_docs)\n",
    "        prompt = f\"\"\"\n",
    "        Legal Answer Validation - Strict Check:\n",
    "\n",
    "        Evaluate the answer based on the provided documents according to these criteria:\n",
    "        - Correctness: Does the answer accurately reflect information from the documents?\n",
    "        - Completeness: Does the answer include all critical and relevant information?\n",
    "        - Honesty: Does the answer avoid making claims not supported by the documents?\n",
    "\n",
    "        If the answer fails any of these criteria, or if critical information is missing, or if unsupported claims are made, consider it invalid.\n",
    "\n",
    "        Documents:\n",
    "        {context_str}\n",
    "\n",
    "        Query: {query}\n",
    "        Answer: {answer}\n",
    "\n",
    "        Respond ONLY with one word: 'valid' or 'invalid'. No explanations.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.strip()}],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            content = response.choices[0].message.content.strip().lower()\n",
    "            return 'valid' in content and 'invalid' not in content\n",
    "        except Exception:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce6644fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_ir_metrics_table(df, ks=[3, 5, 7], col_names=None):\n",
    "    def evaluate_column(col_name, k):\n",
    "        precision_list = []\n",
    "        recall_list = []\n",
    "        rr_list = []\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            gt_source = row[\"Source\"].strip()\n",
    "            gt_category = row[\"Category\"].strip()\n",
    "            gt_pair = (gt_source, gt_category)\n",
    "\n",
    "            retrieved = ast.literal_eval(row[col_name])\n",
    "\n",
    "            top_k = retrieved[:k]\n",
    "            retrieved_pairs = [(doc[0].strip(), doc[1].strip()) for doc in top_k]\n",
    "\n",
    "            relevance = [1 if pair == gt_pair else 0 for pair in retrieved_pairs]\n",
    "\n",
    "            precision = sum(relevance) / k\n",
    "            precision_list.append(precision)\n",
    "\n",
    "            recall = 1 if gt_pair in retrieved_pairs else 0\n",
    "            recall_list.append(recall)\n",
    "\n",
    "            if gt_pair in retrieved_pairs:\n",
    "                rank = retrieved_pairs.index(gt_pair)\n",
    "                rr = 1 / (rank + 1)\n",
    "            else:\n",
    "                rr = 0\n",
    "            rr_list.append(rr)\n",
    "\n",
    "        return {\n",
    "            \"Precision\": np.mean(precision_list),\n",
    "            \"Recall\": np.mean(recall_list),\n",
    "            \"MRR\": np.mean(rr_list)\n",
    "        }\n",
    "\n",
    "    rows = []\n",
    "    for col_name in col_names:\n",
    "        for k in ks:\n",
    "            metrics = evaluate_column(col_name, k)\n",
    "            rows.append({\n",
    "                \"Column\": col_name,\n",
    "                \"k\": k,\n",
    "                **metrics\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbb9af5",
   "metadata": {},
   "source": [
    "1. What embedding model to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "859516af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collection: BAAI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving docs for BAAI: 100%|██████████| 150/150 [02:21<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collection: LegalDocument\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving docs for LegalDocument: 100%|██████████| 150/150 [02:23<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collection: multilingual\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving docs for multilingual: 100%|██████████| 150/150 [02:41<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_path, use_pooling=True):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        if use_pooling:\n",
    "            self.model = AutoModel.from_pretrained(model_path)\n",
    "        else:\n",
    "            self.model = AutoModel.from_pretrained(model_path, add_pooling_layer=False)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs)[0][:, 0] \n",
    "        return torch.nn.functional.normalize(embeddings, p=2, dim=1).squeeze(0).tolist()\n",
    "\n",
    "embedding_models = {\n",
    "    \"BAAI\": {\n",
    "        \"path\": r\"C:\\Users\\Alister\\Desktop\\AI Classes\\Capstone\\Models2\\Embeddings\\BAAI\\bge-base-en-v1.5\",\n",
    "        \"use_pooling\": True\n",
    "    },\n",
    "    \"LegalDocument\": {\n",
    "        \"path\": r\"C:\\Users\\Alister\\Desktop\\AI Classes\\Capstone\\Models2\\Embeddings\\Snowflake\\snowflake-arctic-embed-m\",\n",
    "        \"use_pooling\": False\n",
    "    },\n",
    "    \"multilingual\": {\n",
    "        \"path\": r\"C:\\Users\\Alister\\Desktop\\AI Classes\\Capstone\\Models2\\Embeddings\\intfloat\\multilingual-e5-large-instruct\",\n",
    "        \"use_pooling\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"generated_qa.csv\")\n",
    "\n",
    "for collection_name, settings in embedding_models.items():\n",
    "    model_manager = ModelManager(settings[\"path\"], use_pooling=settings[\"use_pooling\"])\n",
    "    retrieved_docs_list = []\n",
    "    \n",
    "    print(f\"Processing collection: {collection_name}\")\n",
    "    \n",
    "    for question in tqdm(df[\"Question_EN\"], desc=f\"Retrieving docs for {collection_name}\"):\n",
    "        with DocumentRetrievalModule(host=\"localhost\", collection_name=collection_name, alpha=0.5) as searcher:\n",
    "            try:\n",
    "                context_docs = searcher.search_documents(question, max_results=7)\n",
    "                sources = [(doc[\"source\"], doc[\"category\"], doc[\"score\"]) for doc in context_docs]\n",
    "            except Exception:\n",
    "                sources = []\n",
    "        retrieved_docs_list.append(sources)\n",
    "\n",
    "    df[f\"{collection_name}_documents_retrieved\"] = [json.dumps(docs) for docs in retrieved_docs_list]\n",
    "\n",
    "df.to_csv(\"generated_qa_with_docs_all_collections.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5be14790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>k</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAAI_documents_retrieved</td>\n",
       "      <td>3</td>\n",
       "      <td>0.306667</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.707778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAAI_documents_retrieved</td>\n",
       "      <td>5</td>\n",
       "      <td>0.198667</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.718111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAAI_documents_retrieved</td>\n",
       "      <td>7</td>\n",
       "      <td>0.145714</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.722238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snowflake_documents_retrieved</td>\n",
       "      <td>3</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.713333</td>\n",
       "      <td>0.465556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>snowflake_documents_retrieved</td>\n",
       "      <td>5</td>\n",
       "      <td>0.162667</td>\n",
       "      <td>0.806667</td>\n",
       "      <td>0.486889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>snowflake_documents_retrieved</td>\n",
       "      <td>7</td>\n",
       "      <td>0.128571</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.498000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>multilingual_documents_retrieved</td>\n",
       "      <td>3</td>\n",
       "      <td>0.291111</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.667778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>multilingual_documents_retrieved</td>\n",
       "      <td>5</td>\n",
       "      <td>0.189333</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.681444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>multilingual_documents_retrieved</td>\n",
       "      <td>7</td>\n",
       "      <td>0.139048</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.685730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Column  k  Precision    Recall       MRR\n",
       "0          BAAI_documents_retrieved  3   0.306667  0.880000  0.707778\n",
       "1          BAAI_documents_retrieved  5   0.198667  0.926667  0.718111\n",
       "2          BAAI_documents_retrieved  7   0.145714  0.953333  0.722238\n",
       "3     snowflake_documents_retrieved  3   0.240000  0.713333  0.465556\n",
       "4     snowflake_documents_retrieved  5   0.162667  0.806667  0.486889\n",
       "5     snowflake_documents_retrieved  7   0.128571  0.880000  0.498000\n",
       "6  multilingual_documents_retrieved  3   0.291111  0.840000  0.667778\n",
       "7  multilingual_documents_retrieved  5   0.189333  0.900000  0.681444\n",
       "8  multilingual_documents_retrieved  7   0.139048  0.926667  0.685730"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"generated_qa_with_docs_all_collections.csv\")\n",
    "cols_to_eval = [\"BAAI_documents_retrieved\", \"snowflake_documents_retrieved\", \"multilingual_documents_retrieved\"]\n",
    "ks = [3, 5, 7]\n",
    "\n",
    "results_df = evaluate_ir_metrics_table(df, ks=ks, col_names=cols_to_eval)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae84d4",
   "metadata": {},
   "source": [
    "2. multilinguel embedding model vs. translated and english-only embedding model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69a5f949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collection: BAAI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving docs for BAAI: 100%|██████████| 150/150 [04:50<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collection: LegalDocument\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving docs for LegalDocument:  82%|████████▏ | 123/150 [04:12<00:51,  1.92s/it]c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\site-packages\\weaviate\\warnings.py:314: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=3256 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Retrieving docs for LegalDocument: 100%|██████████| 150/150 [05:07<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing collection: multilingual\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving docs for multilingual: 100%|██████████| 150/150 [02:45<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_path, use_pooling=True):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        if use_pooling:\n",
    "            self.model = AutoModel.from_pretrained(model_path)\n",
    "        else:\n",
    "            self.model = AutoModel.from_pretrained(model_path, add_pooling_layer=False)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs)[0][:, 0] \n",
    "        return torch.nn.functional.normalize(embeddings, p=2, dim=1).squeeze(0).tolist()\n",
    "\n",
    "embedding_models = {\n",
    "    \"BAAI\": {\n",
    "        \"path\": r\"C:\\Users\\Alister\\Desktop\\AI Classes\\Capstone\\Models2\\Embeddings\\BAAI\\bge-base-en-v1.5\",\n",
    "        \"use_pooling\": True\n",
    "    },\n",
    "    \"LegalDocument\": {\n",
    "        \"path\": r\"C:\\Users\\Alister\\Desktop\\AI Classes\\Capstone\\Models2\\Embeddings\\Snowflake\\snowflake-arctic-embed-m\",\n",
    "        \"use_pooling\": False\n",
    "    },\n",
    "    \"multilingual\": {\n",
    "        \"path\": r\"C:\\Users\\Alister\\Desktop\\AI Classes\\Capstone\\Models2\\Embeddings\\intfloat\\multilingual-e5-large-instruct\",\n",
    "        \"use_pooling\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"generated_qa.csv\")\n",
    "\n",
    "query_transformation_module = QueryTransformationModule()\n",
    "\n",
    "for collection_name, settings in embedding_models.items():\n",
    "    model_manager = ModelManager(settings[\"path\"], use_pooling=settings[\"use_pooling\"])\n",
    "    retrieved_docs_list = []\n",
    "    \n",
    "    print(f\"Processing collection: {collection_name}\")\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=f\"Retrieving docs for {collection_name}\"):\n",
    "        if collection_name == \"multilingual\":\n",
    "            query = row[\"Question_TL\"]\n",
    "        else:\n",
    "            user_input = row[\"Question_TL\"]\n",
    "            query = query_transformation_module.refine_query_with_history(user_input, [])\n",
    "        \n",
    "        with DocumentRetrievalModule(host=\"localhost\", collection_name=collection_name, alpha=0.5) as searcher:\n",
    "            try:\n",
    "                context_docs = searcher.search_documents(query, max_results=7)\n",
    "                sources = [(doc[\"source\"], doc[\"category\"], doc[\"score\"]) for doc in context_docs]\n",
    "            except Exception as e:\n",
    "                print(f\"Error searching documents for query '{query}': {e}\")\n",
    "                sources = []\n",
    "        retrieved_docs_list.append(sources)\n",
    "\n",
    "    df[f\"{collection_name}_documents_retrieved\"] = [json.dumps(docs) for docs in retrieved_docs_list]\n",
    "\n",
    "df.to_csv(\"generated_qa_expt_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83975707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>k</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAAI_documents_retrieved</td>\n",
       "      <td>3</td>\n",
       "      <td>0.306667</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAAI_documents_retrieved</td>\n",
       "      <td>5</td>\n",
       "      <td>0.193333</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.698000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAAI_documents_retrieved</td>\n",
       "      <td>7</td>\n",
       "      <td>0.141905</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.701333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snowflake_documents_retrieved</td>\n",
       "      <td>3</td>\n",
       "      <td>0.226667</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.443333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>snowflake_documents_retrieved</td>\n",
       "      <td>5</td>\n",
       "      <td>0.162667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.470667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>snowflake_documents_retrieved</td>\n",
       "      <td>7</td>\n",
       "      <td>0.124762</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.479714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>multilingual_documents_retrieved</td>\n",
       "      <td>3</td>\n",
       "      <td>0.208889</td>\n",
       "      <td>0.606667</td>\n",
       "      <td>0.437778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>multilingual_documents_retrieved</td>\n",
       "      <td>5</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.673333</td>\n",
       "      <td>0.453444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>multilingual_documents_retrieved</td>\n",
       "      <td>7</td>\n",
       "      <td>0.108571</td>\n",
       "      <td>0.726667</td>\n",
       "      <td>0.461698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Column  k  Precision    Recall       MRR\n",
       "0          BAAI_documents_retrieved  3   0.306667  0.886667  0.690000\n",
       "1          BAAI_documents_retrieved  5   0.193333  0.920000  0.698000\n",
       "2          BAAI_documents_retrieved  7   0.141905  0.940000  0.701333\n",
       "3     snowflake_documents_retrieved  3   0.226667  0.680000  0.443333\n",
       "4     snowflake_documents_retrieved  5   0.162667  0.800000  0.470667\n",
       "5     snowflake_documents_retrieved  7   0.124762  0.860000  0.479714\n",
       "6  multilingual_documents_retrieved  3   0.208889  0.606667  0.437778\n",
       "7  multilingual_documents_retrieved  5   0.140000  0.673333  0.453444\n",
       "8  multilingual_documents_retrieved  7   0.108571  0.726667  0.461698"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"generated_qa_expt_2.csv\")\n",
    "cols_to_eval = [\"BAAI_documents_retrieved\", \"snowflake_documents_retrieved\", \"multilingual_documents_retrieved\"]\n",
    "ks = [3, 5, 7]\n",
    "\n",
    "results_df = evaluate_ir_metrics_table(df, ks=ks, col_names=cols_to_eval)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb39e2e",
   "metadata": {},
   "source": [
    "3. Hybrid weight to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f2b571b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving docs for BAAI (alpha=0.3): 100%|██████████| 150/150 [02:43<00:00,  1.09s/it]\n",
      "Retrieving docs for BAAI (alpha=0.5): 100%|██████████| 150/150 [03:39<00:00,  1.46s/it]\n",
      "Retrieving docs for BAAI (alpha=0.7): 100%|██████████| 150/150 [02:36<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "COLLECTION_NAME = \"BAAI\"\n",
    "ALPHAS = [0.3, 0.5, 0.7]\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModel.from_pretrained(model_path, add_pooling_layer=True)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs)[0][:, 0]\n",
    "        return torch.nn.functional.normalize(embeddings, p=2, dim=1).squeeze(0).tolist()\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "embedding_path = os.path.join(base_dir, \"embeddings\")\n",
    "model_manager = ModelManager(embedding_path)  \n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"generated_qa.csv\")\n",
    "\n",
    "for ALPHA in ALPHAS:\n",
    "    retrieved_docs_list = []\n",
    "    for question in tqdm(df[\"Question_EN\"], desc=f\"Retrieving docs for {COLLECTION_NAME} (alpha={ALPHA})\"):\n",
    "        with DocumentRetrievalModule(host=\"localhost\", collection_name=COLLECTION_NAME, alpha=ALPHA) as searcher:\n",
    "            context_docs = searcher.search_documents(question, max_results=7)\n",
    "            sources = [(doc[\"source\"], doc[\"category\"], doc[\"score\"]) for doc in context_docs]\n",
    "     \n",
    "        retrieved_docs_list.append(sources)\n",
    "\n",
    "    df[f\"{COLLECTION_NAME}_documents_retrieved_{ALPHA}\"] = [json.dumps(docs) for docs in retrieved_docs_list]\n",
    "\n",
    "df.to_csv(\"generated_qa_expt_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "43ea355a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>k</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BAAI_documents_retrieved_0.3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.697778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAAI_documents_retrieved_0.3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.194667</td>\n",
       "      <td>0.906667</td>\n",
       "      <td>0.707444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAAI_documents_retrieved_0.3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.144762</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.712683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BAAI_documents_retrieved_0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.306667</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.707778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BAAI_documents_retrieved_0.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.198667</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.718111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BAAI_documents_retrieved_0.5</td>\n",
       "      <td>7</td>\n",
       "      <td>0.145714</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.722238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BAAI_documents_retrieved_0.7</td>\n",
       "      <td>3</td>\n",
       "      <td>0.295556</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.685556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BAAI_documents_retrieved_0.7</td>\n",
       "      <td>5</td>\n",
       "      <td>0.197333</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.699222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BAAI_documents_retrieved_0.7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.145714</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.703190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Column  k  Precision    Recall       MRR\n",
       "0  BAAI_documents_retrieved_0.3  3   0.300000  0.866667  0.697778\n",
       "1  BAAI_documents_retrieved_0.3  5   0.194667  0.906667  0.707444\n",
       "2  BAAI_documents_retrieved_0.3  7   0.144762  0.940000  0.712683\n",
       "3  BAAI_documents_retrieved_0.5  3   0.306667  0.880000  0.707778\n",
       "4  BAAI_documents_retrieved_0.5  5   0.198667  0.926667  0.718111\n",
       "5  BAAI_documents_retrieved_0.5  7   0.145714  0.953333  0.722238\n",
       "6  BAAI_documents_retrieved_0.7  3   0.295556  0.866667  0.685556\n",
       "7  BAAI_documents_retrieved_0.7  5   0.197333  0.926667  0.699222\n",
       "8  BAAI_documents_retrieved_0.7  7   0.145714  0.953333  0.703190"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"generated_qa_expt_3.csv\")\n",
    "cols_to_eval = [\"BAAI_documents_retrieved_0.3\", \"BAAI_documents_retrieved_0.5\", \"BAAI_documents_retrieved_0.7\"]\n",
    "ks = [3, 5, 7]\n",
    "\n",
    "results_df = evaluate_ir_metrics_table(df, ks=ks, col_names=cols_to_eval)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d95eff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72496713",
   "metadata": {},
   "source": [
    "4. Query Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8fb54ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "refine_query (gpt-4o): 100%|██████████| 150/150 [05:06<00:00,  2.05s/it]\n",
      "HyDE (gpt-4o): 100%|██████████| 150/150 [14:02<00:00,  5.62s/it]\n",
      "refine_query (gpt-4o-mini): 100%|██████████| 150/150 [05:47<00:00,  2.32s/it]\n",
      "HyDE (gpt-4o-mini): 100%|██████████| 150/150 [16:02<00:00,  6.42s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "query_transformations = [\"refine_query\", \"HyDE\"]\n",
    "models = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModel.from_pretrained(model_path, add_pooling_layer=True)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs)[0][:, 0]\n",
    "        return torch.nn.functional.normalize(embeddings, p=2, dim=1).squeeze(0).tolist()\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "embedding_path = os.path.join(base_dir, \"embeddings\")\n",
    "model_manager = ModelManager(embedding_path)  \n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"generated_qa.csv\")\n",
    "\n",
    "for model_name in models:\n",
    "    query_transformation_module = QueryTransformationModule(\n",
    "        refine_model=model_name, \n",
    "        hyde_model=model_name\n",
    "    )\n",
    "    \n",
    "    for query_transformation in query_transformations:\n",
    "        retrieved_docs_list = []\n",
    "        refined_queries = []\n",
    "\n",
    "        for question in tqdm(df[\"Question_EN\"], desc=f\"{query_transformation} ({model_name})\"):\n",
    "            if query_transformation == \"refine_query\":\n",
    "                refined_query = query_transformation_module.refine_query_with_history(question, context_str=\"\")\n",
    "            else:  # HyDE\n",
    "                refined_query = query_transformation_module.generate_hypothetical_document(question)\n",
    "\n",
    "            refined_queries.append(refined_query)\n",
    "\n",
    "            with DocumentRetrievalModule(host=\"localhost\", collection_name=\"BAAI\", alpha=0.5) as searcher:\n",
    "                try:\n",
    "                    context_docs = searcher.search_documents(refined_query, max_results=7)\n",
    "                    sources = [(doc[\"source\"], doc[\"category\"], doc[\"score\"]) for doc in context_docs]\n",
    "                except Exception:\n",
    "                    sources = []\n",
    "\n",
    "            retrieved_docs_list.append(sources)\n",
    "\n",
    "        refined_col_name = f\"{query_transformation}_refined_queries_{model_name}\"\n",
    "        docs_col_name = f\"{query_transformation}_retrieved_docs_{model_name}\"\n",
    "        \n",
    "        df[refined_col_name] = refined_queries\n",
    "        df[docs_col_name] = [json.dumps(docs) for docs in retrieved_docs_list]\n",
    "\n",
    "df.to_csv(\"generated_qa_expt_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dc6a79fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column</th>\n",
       "      <th>k</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>MRR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>refine_query_retrieved_docs_gpt-4o</td>\n",
       "      <td>3</td>\n",
       "      <td>0.304444</td>\n",
       "      <td>0.886667</td>\n",
       "      <td>0.702222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>refine_query_retrieved_docs_gpt-4o</td>\n",
       "      <td>5</td>\n",
       "      <td>0.194667</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.712556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>refine_query_retrieved_docs_gpt-4o</td>\n",
       "      <td>7</td>\n",
       "      <td>0.141905</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.714619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HyDE_retrieved_docs_gpt-4o</td>\n",
       "      <td>3</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>0.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HyDE_retrieved_docs_gpt-4o</td>\n",
       "      <td>5</td>\n",
       "      <td>0.192000</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.710333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HyDE_retrieved_docs_gpt-4o</td>\n",
       "      <td>7</td>\n",
       "      <td>0.140952</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.714619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>refine_query_retrieved_docs_gpt-4o-mini</td>\n",
       "      <td>3</td>\n",
       "      <td>0.304444</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.713333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>refine_query_retrieved_docs_gpt-4o-mini</td>\n",
       "      <td>5</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.722000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>refine_query_retrieved_docs_gpt-4o-mini</td>\n",
       "      <td>7</td>\n",
       "      <td>0.143810</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.725016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HyDE_retrieved_docs_gpt-4o-mini</td>\n",
       "      <td>3</td>\n",
       "      <td>0.286667</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.673333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HyDE_retrieved_docs_gpt-4o-mini</td>\n",
       "      <td>5</td>\n",
       "      <td>0.185333</td>\n",
       "      <td>0.893333</td>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HyDE_retrieved_docs_gpt-4o-mini</td>\n",
       "      <td>7</td>\n",
       "      <td>0.140952</td>\n",
       "      <td>0.940000</td>\n",
       "      <td>0.694143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Column  k  Precision    Recall       MRR\n",
       "0        refine_query_retrieved_docs_gpt-4o  3   0.304444  0.886667  0.702222\n",
       "1        refine_query_retrieved_docs_gpt-4o  5   0.194667  0.933333  0.712556\n",
       "2        refine_query_retrieved_docs_gpt-4o  7   0.141905  0.946667  0.714619\n",
       "3                HyDE_retrieved_docs_gpt-4o  3   0.280000  0.826667  0.690000\n",
       "4                HyDE_retrieved_docs_gpt-4o  5   0.192000  0.920000  0.710333\n",
       "5                HyDE_retrieved_docs_gpt-4o  7   0.140952  0.946667  0.714619\n",
       "6   refine_query_retrieved_docs_gpt-4o-mini  3   0.304444  0.893333  0.713333\n",
       "7   refine_query_retrieved_docs_gpt-4o-mini  5   0.196000  0.933333  0.722000\n",
       "8   refine_query_retrieved_docs_gpt-4o-mini  7   0.143810  0.953333  0.725016\n",
       "9           HyDE_retrieved_docs_gpt-4o-mini  3   0.286667  0.833333  0.673333\n",
       "10          HyDE_retrieved_docs_gpt-4o-mini  5   0.185333  0.893333  0.687000\n",
       "11          HyDE_retrieved_docs_gpt-4o-mini  7   0.140952  0.940000  0.694143"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"generated_qa_expt_4.csv\")\n",
    "cols_to_eval = [\"refine_query_retrieved_docs_gpt-4o\", \"HyDE_retrieved_docs_gpt-4o\", \"refine_query_retrieved_docs_gpt-4o-mini\", \"HyDE_retrieved_docs_gpt-4o-mini\"]\n",
    "ks = [3, 5, 7]\n",
    "\n",
    "results_df = evaluate_ir_metrics_table(df, ks=ks, col_names=cols_to_eval)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ba242c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afbc6a15",
   "metadata": {},
   "source": [
    "5. Overall Evaluation using english and filipino queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "599f3f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with gpt-4o: 100%|██████████| 150/150 [18:37<00:00,  7.45s/it]\n",
      "Processing with gpt-4o-mini: 100%|██████████| 150/150 [15:43<00:00,  6.29s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"generated_qa.csv\")\n",
    "models = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
    "query_transformation_module = QueryTransformationModule()\n",
    "retrieval_decision_module = RetrievalDecisionModule()\n",
    "agent = AnswerValidationAgent()\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModel.from_pretrained(model_path, add_pooling_layer=True)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs)[0][:, 0]\n",
    "        return torch.nn.functional.normalize(embeddings, p=2, dim=1).squeeze(0).tolist()\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "embedding_path = os.path.join(base_dir, \"embeddings\")\n",
    "model_manager = ModelManager(embedding_path)  \n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "for model_name in models:\n",
    "    response_generator_module = ResponseGeneratorModule(generation_model_with_retrieval=model_name)\n",
    "\n",
    "    for col in [\n",
    "        \"Answer\", \"Source\", \"Category\",\n",
    "        \"final_response\", \"final_sources\", \"final_context\", \"final_valid\",\n",
    "        \"trial_1_query_used\", \"trial_1_response\", \"trial_1_sources\", \"trial_1_verified_context\", \"trial_1_context\", \"trial_1_valid\",\n",
    "        \"trial_2_query_used\", \"trial_2_response\", \"trial_2_sources\", \"trial_2_verified_context\", \"trial_2_context\", \"trial_2_valid\",\n",
    "        \"trial_3_query_used\", \"trial_3_response\", \"trial_3_sources\", \"trial_3_verified_context\", \"trial_3_context\", \"trial_3_valid\"\n",
    "    ]:\n",
    "        df[f\"{col}_{model_name}\"] = None\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing with {model_name}\"):\n",
    "        user_input = row[\"Question_EN\"]\n",
    "\n",
    "        refined_query = query_transformation_module.refine_query_with_history(user_input, context_str=\"\")\n",
    "        trial_data = []\n",
    "        final_response = \"\"\n",
    "        final_sources = []\n",
    "        final_context = []\n",
    "        final_valid = False\n",
    "\n",
    "        with DocumentRetrievalModule(host=\"localhost\", collection_name=\"BAAI\", alpha=0.5) as searcher:\n",
    "            agent = AnswerValidationAgent()\n",
    "            while agent.current_attempt < agent.max_attempts:\n",
    "                trial_query = refined_query\n",
    "                if agent.current_attempt > 0:\n",
    "                    trial_query = query_transformation_module.generate_hypothetical_document(refined_query)\n",
    "\n",
    "                context_docs = searcher.search_documents(trial_query, max_results=7)\n",
    "\n",
    "                sources = [(doc[\"source\"], doc[\"category\"], doc[\"chunk_index\"]) for doc in context_docs]\n",
    "                context_content = [doc[\"text\"] for doc in context_docs]\n",
    "\n",
    "                response, relevant_sources, relevant_contexts = response_generator_module.generate_response(user_input, context_docs, sources)\n",
    "\n",
    "                is_valid = agent.validate_answer(response, relevant_contexts, user_input)\n",
    "\n",
    "                trial_data.append({\n",
    "                    \"query\": trial_query,\n",
    "                    \"response\": response,\n",
    "                    \"sources\": relevant_sources,\n",
    "                    \"context\": context_content,\n",
    "                    \"verified_context\": relevant_contexts,\n",
    "                    \"is_valid\": is_valid\n",
    "                })\n",
    "\n",
    "                if is_valid and relevant_sources:\n",
    "                    final_response = response\n",
    "                    final_sources = relevant_sources\n",
    "                    final_context = context_content\n",
    "                    final_valid = True\n",
    "                    break\n",
    "\n",
    "                agent.current_attempt += 1\n",
    "\n",
    "        df.at[i, f\"Answer_{model_name}\"] = final_response\n",
    "        df.at[i, f\"final_response_{model_name}\"] = final_response\n",
    "        df.at[i, f\"final_sources_{model_name}\"] = json.dumps(final_sources)\n",
    "        df.at[i, f\"final_context_{model_name}\"] = json.dumps(final_context)\n",
    "        df.at[i, f\"final_valid_{model_name}\"] = final_valid\n",
    "\n",
    "        if final_sources:\n",
    "            df.at[i, f\"Source_{model_name}\"] = final_sources[0][0] if final_sources else None\n",
    "            df.at[i, f\"Category_{model_name}\"] = final_sources[0][1] if final_sources else None\n",
    "\n",
    "        for j, trial in enumerate(trial_data):\n",
    "            trial_num = j + 1\n",
    "            df.at[i, f\"trial_{trial_num}_query_used_{model_name}\"] = trial[\"query\"]\n",
    "            df.at[i, f\"trial_{trial_num}_response_{model_name}\"] = trial[\"response\"]\n",
    "            df.at[i, f\"trial_{trial_num}_sources_{model_name}\"] = json.dumps(trial[\"sources\"])\n",
    "            df.at[i, f\"trial_{trial_num}_verified_context_{model_name}\"] = json.dumps(trial[\"verified_context\"])\n",
    "            df.at[i, f\"trial_{trial_num}_context_{model_name}\"] = json.dumps(trial[\"context\"])\n",
    "            df.at[i, f\"trial_{trial_num}_valid_{model_name}\"] = trial[\"is_valid\"]\n",
    "\n",
    "df.to_csv(\"generated_qa_expt_5_final_expt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2b2cbbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  10%|█         | 62/600 [00:45<08:25,  1.06it/s]No statements were generated from the answer.\n",
      "Evaluating:  12%|█▏        | 73/600 [00:53<06:54,  1.27it/s]c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6096 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6244 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=7096 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6376 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6908 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6124 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6612 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6424 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6216 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6976 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6520 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6772 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Evaluating:  17%|█▋        | 101/600 [01:09<03:04,  2.70it/s]No statements were generated from the answer.\n",
      "Evaluating:  52%|█████▏    | 310/600 [04:06<10:02,  2.08s/it]No statements were generated from the answer.\n",
      "Evaluating:  57%|█████▊    | 345/600 [04:33<02:11,  1.94it/s]No statements were generated from the answer.\n",
      "Evaluating:  79%|███████▉  | 476/600 [06:37<01:55,  1.07it/s]Exception raised in Job[532]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-D3BrhOxEyoKAj6MidNYs4AOE on tokens per min (TPM): Limit 200000, Used 198675, Requested 6994. Please try again in 1.7s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  89%|████████▉ | 536/600 [07:37<01:22,  1.30s/it]Exception raised in Job[412]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-D3BrhOxEyoKAj6MidNYs4AOE on tokens per min (TPM): Limit 200000, Used 195494, Requested 6294. Please try again in 536ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating: 100%|██████████| 600/600 [09:40<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS evaluation scores for model gpt-4o:\n",
      "{'faithfulness': 0.9218, 'answer_relevancy': 0.9525, 'answer_similarity': 0.9546, 'answer_correctness': 0.7787}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, answer_similarity, answer_correctness\n",
    "from datasets import Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "df = pd.read_csv(\"generated_qa_expt_5_final_expt.csv\")\n",
    "\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "contexts = []\n",
    "ground_truths = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    questions.append(row.get(\"Question_EN\", \"\"))\n",
    "\n",
    "    gen_answer = row.get(f\"final_response_{model}\", \"\")\n",
    "    answers.append(gen_answer if isinstance(gen_answer, str) else \"\")\n",
    "\n",
    "    try:\n",
    "        context = json.loads(row.get(f\"final_context_{model}\", \"[]\"))\n",
    "    except:\n",
    "        context = []\n",
    "    contexts.append(context)\n",
    "\n",
    "    # Ground truth answer in English\n",
    "    ground_truths.append(row.get(\"Answer\", \"\"))\n",
    "\n",
    "# Prepare dataset for RAGAS\n",
    "data_samples = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truths,\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# Evaluate\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, answer_similarity, answer_correctness]\n",
    ")\n",
    "\n",
    "print(f\"RAGAS evaluation scores for model {model}:\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ddc7b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  38%|███▊      | 228/600 [02:43<04:50,  1.28it/s]No statements were generated from the answer.\n",
      "Evaluating:  55%|█████▍    | 328/600 [04:22<02:17,  1.98it/s]c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6976 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6924 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=7632 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Evaluating:  55%|█████▌    | 330/600 [04:23<02:47,  1.61it/s]Exception raised in Job[413]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-D3BrhOxEyoKAj6MidNYs4AOE on tokens per min (TPM): Limit 200000, Used 198858, Requested 5897. Please try again in 1.426s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  69%|██████▉   | 414/600 [05:39<02:38,  1.17it/s]Exception raised in Job[153]: TimeoutError()\n",
      "Evaluating:  88%|████████▊ | 526/600 [07:41<00:54,  1.36it/s]Exception raised in Job[240]: TimeoutError()\n",
      "Evaluating:  89%|████████▉ | 535/600 [07:48<00:48,  1.34it/s]No statements were generated from the answer.\n",
      "Evaluating:  90%|█████████ | 540/600 [07:53<00:51,  1.16it/s]Exception raised in Job[329]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-D3BrhOxEyoKAj6MidNYs4AOE on tokens per min (TPM): Limit 200000, Used 194469, Requested 7619. Please try again in 626ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  98%|█████████▊| 585/600 [08:38<00:12,  1.22it/s]Exception raised in Job[584]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-D3BrhOxEyoKAj6MidNYs4AOE on tokens per min (TPM): Limit 200000, Used 197587, Requested 7136. Please try again in 1.416s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating: 100%|██████████| 600/600 [09:06<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS evaluation scores for model gpt-4o-mini:\n",
      "{'faithfulness': 0.8921, 'answer_relevancy': 0.9454, 'answer_similarity': 0.9580, 'answer_correctness': 0.7858}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, answer_similarity, answer_correctness\n",
    "from datasets import Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "df = pd.read_csv(\"generated_qa_expt_5_final_expt.csv\")\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "contexts = []\n",
    "ground_truths = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    questions.append(row[\"Question_EN\"])\n",
    "\n",
    "    # Generated answer for this model (final_response)\n",
    "    gen_answer = row.get(f\"final_response_{model}\", \"\")\n",
    "    answers.append(gen_answer if isinstance(gen_answer, str) else \"\")\n",
    "\n",
    "    # Context (load JSON if possible)\n",
    "    try:\n",
    "        context = json.loads(row.get(f\"final_context_{model}\", \"[]\"))\n",
    "    except:\n",
    "        context = []\n",
    "    contexts.append(context)\n",
    "\n",
    "    # Ground truth answer (original reference)\n",
    "    ground_truths.append(row.get(\"Answer\", \"\"))\n",
    "\n",
    "# Prepare dataset for RAGAS\n",
    "data_samples = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truths,\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# Evaluate\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, answer_similarity, answer_correctness]\n",
    ")\n",
    "\n",
    "print(f\"RAGAS evaluation scores for model {model}:\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "462df743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with gpt-4o: 100%|██████████| 150/150 [27:30<00:00, 11.01s/it]\n",
      "Processing with gpt-4o-mini: 100%|██████████| 150/150 [35:31<00:00, 14.21s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from openai import OpenAI\n",
    "\n",
    "df = pd.read_csv(\"generated_qa.csv\")\n",
    "models = [\"gpt-4o\", \"gpt-4o-mini\"]\n",
    "query_transformation_module = QueryTransformationModule()\n",
    "retrieval_decision_module = RetrievalDecisionModule()\n",
    "agent = AnswerValidationAgent()\n",
    "\n",
    "class ModelManager:\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModel.from_pretrained(model_path, add_pooling_layer=True)\n",
    "\n",
    "    def get_embedding(self, text):\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(**inputs)[0][:, 0]\n",
    "        return torch.nn.functional.normalize(embeddings, p=2, dim=1).squeeze(0).tolist()\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "embedding_path = os.path.join(base_dir, \"embeddings\")\n",
    "model_manager = ModelManager(embedding_path)  \n",
    "client = OpenAI()\n",
    "\n",
    "def translate_to_english(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful translator from Tagalog to English.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Translate this text to English:\\n\\n{text}\"}\n",
    "        ],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "for model_name in models:\n",
    "    response_generator_module = ResponseGeneratorModule(generation_model_with_retrieval=model_name)\n",
    "\n",
    "    for col in [\n",
    "        \"Answer\", \"Source\", \"Category\",\n",
    "        \"final_response\", \"final_sources\", \"final_context\", \"final_valid\",\n",
    "        \"trial_1_query_used\", \"trial_1_response\", \"trial_1_sources\", \"trial_1_verified_context\", \"trial_1_context\", \"trial_1_valid\",\n",
    "        \"trial_2_query_used\", \"trial_2_response\", \"trial_2_sources\", \"trial_2_verified_context\", \"trial_2_context\", \"trial_2_valid\",\n",
    "        \"trial_3_query_used\", \"trial_3_response\", \"trial_3_sources\", \"trial_3_verified_context\", \"trial_3_context\", \"trial_3_valid\"\n",
    "    ]:\n",
    "        df[f\"{col}_{model_name}\"] = None\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing with {model_name}\"):\n",
    "        user_input = row[\"Question_TL\"] \n",
    "\n",
    "        refined_query = query_transformation_module.refine_query_with_history(user_input, context_str=\"\")\n",
    "        trial_data = []\n",
    "        final_response = \"\"\n",
    "        final_sources = []\n",
    "        final_context = []\n",
    "        final_valid = False\n",
    "\n",
    "        with DocumentRetrievalModule(host=\"localhost\", collection_name=\"BAAI\", alpha=0.5) as searcher:\n",
    "            agent = AnswerValidationAgent()\n",
    "            while agent.current_attempt < agent.max_attempts:\n",
    "                trial_query = refined_query\n",
    "                if agent.current_attempt > 0:\n",
    "                    trial_query = query_transformation_module.generate_hypothetical_document(refined_query)\n",
    "\n",
    "                context_docs = searcher.search_documents(trial_query, max_results=7)\n",
    "\n",
    "                sources = [(doc[\"source\"], doc[\"category\"], doc[\"chunk_index\"]) for doc in context_docs]\n",
    "                context_content = [doc[\"text\"] for doc in context_docs]\n",
    "\n",
    "                response, relevant_sources, relevant_contexts = response_generator_module.generate_response(user_input, context_docs, sources)\n",
    "\n",
    "                is_valid = agent.validate_answer(response, relevant_contexts, user_input)\n",
    "\n",
    "                trial_data.append({\n",
    "                    \"query\": trial_query,\n",
    "                    \"response\": response,\n",
    "                    \"sources\": relevant_sources,\n",
    "                    \"context\": context_content,\n",
    "                    \"verified_context\": relevant_contexts,\n",
    "                    \"is_valid\": is_valid\n",
    "                })\n",
    "\n",
    "                if is_valid and relevant_sources:\n",
    "                    final_response = response\n",
    "                    final_sources = relevant_sources\n",
    "                    final_context = context_content\n",
    "                    final_valid = True\n",
    "                    break\n",
    "\n",
    "                agent.current_attempt += 1\n",
    "\n",
    "        # Translate final response to English\n",
    "        final_answer_en = translate_to_english(final_response) if final_response else \"\"\n",
    "\n",
    "        df.at[i, f\"Answer_{model_name}\"] = final_response  # Tagalog answer\n",
    "        df.at[i, f\"final_response_{model_name}\"] = final_response  # Tagalog final response\n",
    "        df.at[i, f\"final_answer_en_{model_name}\"] = final_answer_en  # English translation of final answer\n",
    "        df.at[i, f\"final_sources_{model_name}\"] = json.dumps(final_sources)\n",
    "        df.at[i, f\"final_context_{model_name}\"] = json.dumps(final_context)\n",
    "        df.at[i, f\"final_valid_{model_name}\"] = final_valid\n",
    "\n",
    "        if final_sources:\n",
    "            df.at[i, f\"Source_{model_name}\"] = final_sources[0][0] if final_sources else None\n",
    "            df.at[i, f\"Category_{model_name}\"] = final_sources[0][1] if final_sources else None\n",
    "\n",
    "        for j, trial in enumerate(trial_data):\n",
    "            trial_num = j + 1\n",
    "            df.at[i, f\"trial_{trial_num}_query_used_{model_name}\"] = trial[\"query\"]\n",
    "            df.at[i, f\"trial_{trial_num}_response_{model_name}\"] = trial[\"response\"]\n",
    "            df.at[i, f\"trial_{trial_num}_sources_{model_name}\"] = json.dumps(trial[\"sources\"])\n",
    "            df.at[i, f\"trial_{trial_num}_verified_context_{model_name}\"] = json.dumps(trial[\"verified_context\"])\n",
    "            df.at[i, f\"trial_{trial_num}_context_{model_name}\"] = json.dumps(trial[\"context\"])\n",
    "            df.at[i, f\"trial_{trial_num}_valid_{model_name}\"] = trial[\"is_valid\"]\n",
    "\n",
    "df.to_csv(\"generated_qa_expt_5_tagalog_final_expt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54709150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  12%|█▏        | 69/600 [00:58<09:54,  1.12s/it]No statements were generated from the answer.\n",
      "Evaluating:  46%|████▌     | 274/600 [03:32<04:40,  1.16it/s]No statements were generated from the answer.\n",
      "Evaluating:  55%|█████▌    | 332/600 [04:41<03:41,  1.21it/s]Exception raised in Job[316]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-D3BrhOxEyoKAj6MidNYs4AOE on tokens per min (TPM): Limit 200000, Used 195496, Requested 9197. Please try again in 1.407s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  58%|█████▊    | 348/600 [04:58<05:41,  1.36s/it]Exception raised in Job[137]: TimeoutError()\n",
      "Evaluating:  61%|██████    | 364/600 [05:04<02:03,  1.91it/s]No statements were generated from the answer.\n",
      "Evaluating:  69%|██████▉   | 413/600 [05:50<02:59,  1.04it/s]c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=8076 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=6572 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=8168 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=8092 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=7384 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Evaluating:  87%|████████▋ | 524/600 [07:42<02:12,  1.74s/it]Exception raised in Job[232]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 600/600 [09:58<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS evaluation scores for model gpt-4o-mini for Tagalog questions:\n",
      "{'faithfulness': 0.8648, 'answer_relevancy': 0.9366, 'answer_similarity': 0.9509, 'answer_correctness': 0.7405}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, answer_similarity, answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(\"generated_qa_expt_5_tagalog_final_expt.csv\")\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "contexts = []\n",
    "ground_truths = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    questions.append(row.get(\"Question_EN\", \"\"))\n",
    "\n",
    "    gen_answer = row.get(f\"final_answer_en_{model}\", \"\")\n",
    "    answers.append(gen_answer if isinstance(gen_answer, str) else \"\")\n",
    "\n",
    "    try:\n",
    "        context = json.loads(row.get(f\"final_context_{model}\", \"[]\"))\n",
    "    except:\n",
    "        context = []\n",
    "    contexts.append(context)\n",
    "\n",
    "    # Ground truth answer in English\n",
    "    ground_truths.append(row.get(\"Answer\", \"\"))\n",
    "\n",
    "# Prepare dataset for RAGAS\n",
    "data_samples = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truths,\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# Evaluate\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, answer_similarity, answer_correctness]\n",
    ")\n",
    "\n",
    "print(f\"RAGAS evaluation scores for model {model} for Tagalog questions:\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36a213bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  10%|▉         | 58/600 [00:36<02:26,  3.69it/s]No statements were generated from the answer.\n",
      "Evaluating:  28%|██▊       | 166/600 [01:41<05:55,  1.22it/s]No statements were generated from the answer.\n",
      "Evaluating:  66%|██████▌   | 395/600 [05:24<02:50,  1.20it/s]Exception raised in Job[233]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-D3BrhOxEyoKAj6MidNYs4AOE on tokens per min (TPM): Limit 200000, Used 197888, Requested 9681. Please try again in 2.27s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  66%|██████▋   | 399/600 [05:33<06:36,  1.97s/it]Exception raised in Job[316]: RateLimitError(Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4o-mini in organization org-D3BrhOxEyoKAj6MidNYs4AOE on tokens per min (TPM): Limit 200000, Used 193982, Requested 9194. Please try again in 952ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}})\n",
      "Evaluating:  80%|████████  | 482/600 [06:44<02:13,  1.13s/it]Exception raised in Job[152]: TimeoutError()\n",
      "Evaluating:  86%|████████▌ | 513/600 [07:16<01:39,  1.15s/it]Exception raised in Job[128]: TimeoutError()\n",
      "Evaluating:  96%|█████████▋| 579/600 [08:20<00:19,  1.05it/s]Exception raised in Job[153]: TimeoutError()\n",
      "Evaluating: 100%|██████████| 600/600 [08:56<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS evaluation scores for model gpt-4o for Tagalog questions:\n",
      "{'faithfulness': 0.9213, 'answer_relevancy': 0.9518, 'answer_similarity': 0.9555, 'answer_correctness': 0.7664}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, answer_similarity, answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(\"generated_qa_expt_5_tagalog_final_expt.csv\")\n",
    "\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "contexts = []\n",
    "ground_truths = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    questions.append(row.get(\"Question_EN\", \"\"))\n",
    "\n",
    "    gen_answer = row.get(f\"final_answer_en_{model}\", \"\")\n",
    "    answers.append(gen_answer if isinstance(gen_answer, str) else \"\")\n",
    "\n",
    "    try:\n",
    "        context = json.loads(row.get(f\"final_context_{model}\", \"[]\"))\n",
    "    except:\n",
    "        context = []\n",
    "    contexts.append(context)\n",
    "\n",
    "    # Ground truth answer in English\n",
    "    ground_truths.append(row.get(\"Answer\", \"\"))\n",
    "\n",
    "# Prepare dataset for RAGAS\n",
    "data_samples = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"contexts\": contexts,\n",
    "    \"ground_truth\": ground_truths,\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# Evaluate\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    metrics=[faithfulness, answer_relevancy, answer_similarity, answer_correctness]\n",
    ")\n",
    "\n",
    "print(f\"RAGAS evaluation scores for model {model} for Tagalog questions:\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ba09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fb2210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseGeneratorWithoutRetrievalModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client=client,\n",
    "        generation_model_without_retrieval=\"gpt-4o-mini\",\n",
    "        generation_temperature=0.1,\n",
    "        max_tokens=512\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.generation_model_without_retrieval = generation_model_without_retrieval\n",
    "        self.generation_temperature = generation_temperature\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def conversation_without_retrieval(self, user_input, context_str=None):\n",
    "        prompt = f\"\"\"\n",
    "        You are a Quezon City Legal Provider. Answer the query using your internal knowledge.\n",
    "\n",
    "        User query:\n",
    "        {user_input}\n",
    "\n",
    "        Please answer clearly and accurately.  \n",
    "        Note: Provide the answer in English.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.generation_model_without_retrieval,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.strip()}],\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception:\n",
    "            return \"Error generating response\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58f885dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with gpt-4o-mini: 100%|██████████| 150/150 [15:02<00:00,  6.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"generated_qa.csv\")\n",
    "models = [\"gpt-4o-mini\"]\n",
    "\n",
    "response_generator_module = ResponseGeneratorWithoutRetrievalModule()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "for model_name in models:\n",
    "    response_generator_module = ResponseGeneratorWithoutRetrievalModule(generation_model_without_retrieval=model_name)\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing with {model_name}\"):\n",
    "        user_input = row[\"Question_EN\"]\n",
    "\n",
    "        generated_response = response_generator_module.conversation_without_retrieval(user_input)\n",
    "        df.at[i, f\"generated_response_{model_name}\"] = generated_response\n",
    "\n",
    "df.to_csv(\"generated_qa_expt_5_english_no_retrieval.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b18499c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 300/300 [05:38<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS evaluation scores for no RAG:\n",
      "{'answer_similarity': 0.9324, 'answer_correctness': 0.6024}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, answer_similarity, answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(\"generated_qa_expt_5_english_no_retrieval.csv\")\n",
    "\n",
    "model = \"gpt-4o-mini\"\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "ground_truths = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    questions.append(row.get(\"Question_EN\", \"\"))\n",
    "\n",
    "    gen_answer = row.get(f\"generated_response_{model}\", \"\")\n",
    "    answers.append(gen_answer if isinstance(gen_answer, str) else \"\")\n",
    "\n",
    "    # Ground truth answer in English\n",
    "    ground_truths.append(row.get(\"Answer\", \"\"))\n",
    "\n",
    "# Prepare dataset for RAGAS\n",
    "data_samples = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"ground_truth\": ground_truths,\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# Evaluate\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    metrics=[answer_similarity, answer_correctness]\n",
    ")\n",
    "\n",
    "print(f\"RAGAS evaluation scores for no RAG:\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8faf69b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with gpt-4o: 100%|██████████| 150/150 [18:25<00:00,  7.37s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"generated_qa.csv\")\n",
    "models = [\"gpt-4o\"]\n",
    "\n",
    "response_generator_module = ResponseGeneratorWithoutRetrievalModule()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "for model_name in models:\n",
    "    response_generator_module = ResponseGeneratorWithoutRetrievalModule(generation_model_without_retrieval=model_name)\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing with {model_name}\"):\n",
    "        user_input = row[\"Question_EN\"]\n",
    "\n",
    "        generated_response = response_generator_module.conversation_without_retrieval(user_input)\n",
    "        df.at[i, f\"generated_response_{model_name}\"] = generated_response\n",
    "\n",
    "df.to_csv(\"generated_qa_expt_5_english_no_retrieval_4o.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b40892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  45%|████▍     | 134/300 [02:09<04:16,  1.55s/it]c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=7448 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "c:\\Users\\Alister\\anaconda3\\envs\\capstoneenv\\Lib\\asyncio\\selector_events.py:868: ResourceWarning: unclosed transport <_SelectorSocketTransport fd=7488 read=idle write=<idle, bufsize=0>>\n",
      "  _warn(f\"unclosed transport {self!r}\", ResourceWarning, source=self)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Evaluating: 100%|██████████| 300/300 [05:12<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGAS evaluation scores for no RAG:\n",
      "{'answer_similarity': 0.9316, 'answer_correctness': 0.6073}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, answer_similarity, answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(\"generated_qa_expt_5_english_no_retrieval_4o.csv\")\n",
    "\n",
    "model = \"gpt-4o\"\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "ground_truths = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    questions.append(row.get(\"Question_EN\", \"\"))\n",
    "\n",
    "    gen_answer = row.get(f\"generated_response_{model}\", \"\")\n",
    "    answers.append(gen_answer if isinstance(gen_answer, str) else \"\")\n",
    "\n",
    "    # Ground truth answer in English\n",
    "    ground_truths.append(row.get(\"Answer\", \"\"))\n",
    "\n",
    "# Prepare dataset for RAGAS\n",
    "data_samples = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"ground_truth\": ground_truths,\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# Evaluate\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    metrics=[answer_similarity, answer_correctness]\n",
    ")\n",
    "\n",
    "print(f\"RAGAS evaluation scores for no RAG:\")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f08fc57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3062fa09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseGeneratorWithoutRetrievalModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        client=client,\n",
    "        generation_model_without_retrieval=\"gpt-4o-mini\",\n",
    "        max_tokens=512\n",
    "    ):\n",
    "        self.client = client\n",
    "        self.generation_model_without_retrieval = generation_model_without_retrieval\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    def conversation_without_retrieval(self, user_input, context_str=None):\n",
    "        prompt = f\"\"\"\n",
    "        You are a Quezon City Legal Provider. Answer the query using your internal knowledge.\n",
    "\n",
    "        User query:\n",
    "        {user_input}\n",
    "\n",
    "        Please answer clearly and accurately.  \n",
    "        Note: Provide the answer in English.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.generation_model_without_retrieval,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt.strip()}],\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content.strip()\n",
    "        except Exception:\n",
    "            return \"Error generating response\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67ec4cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with gpt-4o-mini-search-preview: 100%|██████████| 150/150 [10:18<00:00,  4.13s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "df = pd.read_csv(\"generated_qa.csv\")\n",
    "models = [\"gpt-4o-mini-search-preview\"]\n",
    "\n",
    "response_generator_module = ResponseGeneratorWithoutRetrievalModule()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "for model_name in models:\n",
    "    response_generator_module = ResponseGeneratorWithoutRetrievalModule(generation_model_without_retrieval=model_name)\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), total=len(df), desc=f\"Processing with {model_name}\"):\n",
    "        user_input = row[\"Question_EN\"]\n",
    "\n",
    "        generated_response = response_generator_module.conversation_without_retrieval(user_input)\n",
    "        df.at[i, f\"generated_response_{model_name}\"] = generated_response\n",
    "\n",
    "df.to_csv(\"generated_qa_expt_5_english_no_retrieval_web_search.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18973a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, answer_similarity, answer_correctness\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(\"generated_qa_expt_5_english_no_retrieval_web_search.csv\")\n",
    "\n",
    "model = \"gpt-4o-mini-search-preview\"\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "ground_truths = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    questions.append(row.get(\"Question_EN\", \"\"))\n",
    "\n",
    "    gen_answer = row.get(f\"generated_response_{model}\", \"\")\n",
    "    answers.append(gen_answer if isinstance(gen_answer, str) else \"\")\n",
    "\n",
    "    # Ground truth answer in English\n",
    "    ground_truths.append(row.get(\"Answer\", \"\"))\n",
    "\n",
    "# Prepare dataset for RAGAS\n",
    "data_samples = {\n",
    "    \"question\": questions,\n",
    "    \"answer\": answers,\n",
    "    \"ground_truth\": ground_truths,\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_samples)\n",
    "\n",
    "# Evaluate\n",
    "score = evaluate(\n",
    "    dataset,\n",
    "    metrics=[answer_similarity, answer_correctness]\n",
    ")\n",
    "\n",
    "print(f\"RAGAS evaluation scores for no RAG (web search):\")\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstoneenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
